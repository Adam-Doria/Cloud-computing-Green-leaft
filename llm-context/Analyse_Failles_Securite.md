# Analyse des Failles de S√©curit√© - Architecture GreenLeaf

**Projet :** GreenLeaf - Plateforme E-commerce

**Date :** 06 Janvier 2026

**Version :** 1.0

---

## 1. R√©sum√© Ex√©cutif

Ce document identifie les **failles potentielles** de l'architecture GreenLeaf et propose des **contre-mesures** pour chacune. L'objectif est de pr√©parer les r√©ponses aux questions du jury et d'anticiper les risques en production.

| Cat√©gorie | Failles Identifi√©es | Niveau de Risque |
|:----------|:--------------------|:-----------------|
| R√©seau | 3 | üü° Moyen |
| Compute | 4 | üü† Moyen-Haut |
| Donn√©es | 3 | üî¥ √âlev√© |
| Op√©rations | 3 | üü° Moyen |
| **Performance / Charge** | **5** | **üî¥ √âlev√©** |

---

## 2. Failles R√©seau

### 2.1. NAT Instance = Single Point of Failure (SPOF)

**üî¥ Le Probl√®me :**
Contrairement √† la NAT Gateway (service manag√© AWS), notre NAT Instance peut tomber en panne (crash Linux, probl√®me hardware). Si elle tombe, les instances priv√©es perdent l'acc√®s Internet (mises √† jour, appels API externes, Stripe, etc.).

**‚úÖ La Solution :**
```hcl
# Auto Scaling Group de taille 1 pour auto-healing
resource "aws_autoscaling_group" "nat" {
  name                = "nat-instance-asg"
  min_size            = 1
  max_size            = 1
  desired_capacity    = 1
  vpc_zone_identifier = [aws_subnet.public_a.id]
  
  launch_template {
    id      = aws_launch_template.nat.id
    version = "$Latest"
  }

  tag {
    key                 = "Name"
    value               = "NAT-Instance"
    propagate_at_launch = true
  }
}
```

**D√©fense orale :**
> "La NAT Instance est encapsul√©e dans un Auto Scaling Group de taille 1. En cas de panne, AWS la remplace automatiquement en moins de 2 minutes. Le trafic client n'est pas impact√© car il passe par l'ALB, pas par la NAT."

---

### 2.2. ALB expos√© uniquement aux IPs Cloudflare

**üü° Le Probl√®me :**
Si un attaquant d√©couvre l'IP de l'ALB (scan, DNS leak), il peut bypasser Cloudflare et attaquer directement l'ALB sans protection WAF.

**‚úÖ La Solution :**
```hcl
# Security Group ALB - Autoriser UNIQUEMENT les IPs Cloudflare
resource "aws_security_group_rule" "alb_cloudflare_only" {
  type              = "ingress"
  from_port         = 80
  to_port           = 80
  protocol          = "tcp"
  security_group_id = aws_security_group.alb.id
  
  # Liste officielle des IPs Cloudflare
  cidr_blocks = [
    "173.245.48.0/20",
    "103.21.244.0/22",
    "103.22.200.0/22",
    "103.31.4.0/22",
    "141.101.64.0/18",
    "108.162.192.0/18",
    "190.93.240.0/20",
    "188.114.96.0/20",
    "197.234.240.0/22",
    "198.41.128.0/17",
    "162.158.0.0/15",
    "104.16.0.0/13",
    "104.24.0.0/14",
    "172.64.0.0/13",
    "131.0.72.0/22"
  ]
}
```

**‚ö†Ô∏è Attention :** Ces IPs doivent √™tre mises √† jour p√©riodiquement (Cloudflare les publie sur [cloudflare.com/ips](https://cloudflare.com/ips)).

---

### 2.3. Pas de WAF AWS (D√©pendance Cloudflare)

**üü° Le Probl√®me :**
Nous d√©pendons enti√®rement de Cloudflare pour le WAF. Si Cloudflare a une panne ou une faille, nous sommes expos√©s.

**‚úÖ La Solution :**
- **Court terme (Budget limit√©)** : Activer les r√®gles Cloudflare Managed Ruleset (gratuit sur plan Pro).
- **Moyen terme** : Ajouter AWS WAF sur l'ALB (~5$/mois + 0.60$/million de requ√™tes).

```hcl
resource "aws_wafv2_web_acl" "main" {
  name  = "greenleaf-waf"
  scope = "REGIONAL"

  default_action {
    allow {}
  }

  rule {
    name     = "AWSManagedRulesCommonRuleSet"
    priority = 1
    
    override_action {
      none {}
    }

    statement {
      managed_rule_group_statement {
        vendor_name = "AWS"
        name        = "AWSManagedRulesCommonRuleSet"
      }
    }

    visibility_config {
      sampled_requests_enabled   = true
      cloudwatch_metrics_enabled = true
      metric_name                = "CommonRuleSet"
    }
  }

  visibility_config {
    cloudwatch_metrics_enabled = true
    metric_name                = "greenleaf-waf"
    sampled_requests_enabled   = true
  }
}
```

---

## 3. Failles Compute (EC2)

### 3.1. Secrets en clair dans User Data

**üî¥ Le Probl√®me :**
Le script `user_data` peut contenir des variables d'environnement sensibles (cl√©s API Stripe, credentials DB). Ces donn√©es sont visibles dans les m√©tadonn√©es EC2.

**‚úÖ La Solution :**
Utiliser **AWS Secrets Manager** ou **SSM Parameter Store** :

```bash
#!/bin/bash
# Dans user_data : r√©cup√©rer les secrets depuis SSM
DB_PASSWORD=$(aws ssm get-parameter --name "/greenleaf/prod/db_password" --with-decryption --query "Parameter.Value" --output text)
STRIPE_KEY=$(aws ssm get-parameter --name "/greenleaf/prod/stripe_key" --with-decryption --query "Parameter.Value" --output text)

# Injecter dans le fichier .env
cat > /opt/app/.env << EOF
DATABASE_URL=postgres://medusa:${DB_PASSWORD}@rds-endpoint:5432/medusa
STRIPE_API_KEY=${STRIPE_KEY}
EOF
```

**Co√ªt SSM Parameter Store :** Gratuit (Standard) ou 0.05$/param√®tre/mois (Advanced).

---

### 3.2. Pas de Bastion / Acc√®s SSH direct impossible

**üü° Le Probl√®me :**
Les instances sont dans des subnets priv√©s. Comment les d√©bugger en cas de probl√®me ?

**‚úÖ La Solution :**
Utiliser **AWS Systems Manager Session Manager** (pas de bastion n√©cessaire) :

```hcl
# IAM Role pour EC2 avec SSM
resource "aws_iam_role_policy_attachment" "ssm" {
  role       = aws_iam_role.ec2_app.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}
```

**Connexion :**
```bash
aws ssm start-session --target i-0123456789abcdef0
```

**Avantages :**
- Pas de port 22 ouvert
- Logs d'audit dans CloudWatch
- Aucun co√ªt suppl√©mentaire

---

### 3.3. Images Docker non scann√©es

**üü° Le Probl√®me :**
Les images Docker peuvent contenir des vuln√©rabilit√©s (CVE) dans les d√©pendances Node.js ou les packages syst√®me.

**‚úÖ La Solution :**
Int√©grer un scan de vuln√©rabilit√©s dans le CI/CD :

```yaml
# GitHub Actions
- name: Scan Docker image
  uses: aquasecurity/trivy-action@master
  with:
    image-ref: 'greenleaf/medusa:${{ github.sha }}'
    format: 'table'
    exit-code: '1'  # Fail si vuln√©rabilit√©s critiques
    severity: 'CRITICAL,HIGH'
```

---

### 3.4. Instances Spot en DEV = Interruptions possibles

**üü° Le Probl√®me :**
AWS peut reprendre les instances Spot √† tout moment avec un pr√©avis de 2 minutes.

**‚úÖ La Solution :**
- Accepter le risque en DEV (c'est le but du low-cost).
- Configurer une notification Spot :

```hcl
resource "aws_spot_instance_request" "dev" {
  # ...
  instance_interruption_behavior = "stop"  # Arr√™ter plut√¥t que terminer
  
  # Notification 2 min avant interruption
  # Les m√©tadonn√©es EC2 contiennent l'avertissement
}
```

**D√©fense orale :**
> "En DEV, l'interruption Spot est acceptable. Le code est versionn√© dans Git, l'√©tat est dans la DB. On peut relancer Terraform en 5 minutes."

---

## 4. Failles Donn√©es

### 4.1. RDS accessible depuis toutes les instances App

**üü† Le Probl√®me :**
Si une instance EC2 est compromise, l'attaquant a acc√®s direct √† la base de donn√©es.

**‚úÖ La Solution :**
1. **Least Privilege** : L'utilisateur Medusa ne doit avoir que les droits n√©cessaires (pas `SUPERUSER`).
2. **Rotation des credentials** : Utiliser RDS Secrets Manager rotation.

```sql
-- Cr√©er un utilisateur limit√©
CREATE USER medusa_app WITH PASSWORD 'xxx';
GRANT CONNECT ON DATABASE medusa TO medusa_app;
GRANT USAGE ON SCHEMA public TO medusa_app;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO medusa_app;
-- PAS de DROP, CREATE, ALTER
```

---

### 4.2. Pas de backup automatique test√©

**üî¥ Le Probl√®me :**
RDS fait des snapshots automatiques, mais personne n'a jamais test√© la restauration.

**‚úÖ La Solution :**
1. Tester la restauration une fois par mois en PREPROD.
2. Documenter la proc√©dure de restauration :

```bash
# Restaurer un snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier greenleaf-restored \
  --db-snapshot-identifier rds:greenleaf-prod-2026-01-06-03-00
```

**Point cl√© pour l'oral :**
> "Un backup qui n'a jamais √©t√© test√© n'est pas un backup. Nous planifions un test de restauration mensuel."

---

### 4.3. Redis sans authentification

**üü† Le Probl√®me :**
Par d√©faut, ElastiCache Redis n'a pas de mot de passe. Toute instance dans le VPC peut s'y connecter.

**‚úÖ La Solution :**
Activer l'authentification Redis (AUTH) :

```hcl
resource "aws_elasticache_replication_group" "redis" {
  # ...
  transit_encryption_enabled = true
  auth_token                 = var.redis_auth_token  # Depuis SSM
  at_rest_encryption_enabled = true
}
```

**Attention :** Medusa doit √™tre configur√© avec le token :
```env
REDIS_URL=rediss://:TOKEN@redis-endpoint:6379
```

---

## 5. Failles Op√©rationnelles

### 5.1. Pas de monitoring des co√ªts en temps r√©el

**üü° Le Probl√®me :**
On peut d√©passer le budget de 500$ sans le savoir jusqu'√† la fin du mois.

**‚úÖ La Solution :**
Configurer AWS Budgets avec alertes :

```hcl
resource "aws_budgets_budget" "monthly" {
  name         = "greenleaf-monthly"
  budget_type  = "COST"
  limit_amount = "500"
  limit_unit   = "USD"
  time_unit    = "MONTHLY"

  notification {
    comparison_operator        = "GREATER_THAN"
    threshold                  = 80
    threshold_type             = "PERCENTAGE"
    notification_type          = "ACTUAL"
    subscriber_email_addresses = ["devops@greenleaf.com"]
  }

  notification {
    comparison_operator        = "GREATER_THAN"
    threshold                  = 100
    threshold_type             = "PERCENTAGE"
    notification_type          = "FORECASTED"
    subscriber_email_addresses = ["devops@greenleaf.com"]
  }
}
```

---

### 5.2. Logs non centralis√©s

**üü° Le Probl√®me :**
Si une instance est termin√©e par l'ASG, ses logs locaux sont perdus.

**‚úÖ La Solution :**
Envoyer les logs vers CloudWatch Logs :

```bash
# Dans user_data
yum install -y amazon-cloudwatch-agent
cat > /opt/aws/amazon-cloudwatch-agent/etc/config.json << 'EOF'
{
  "logs": {
    "logs_collected": {
      "files": {
        "collect_list": [
          {
            "file_path": "/var/log/messages",
            "log_group_name": "/greenleaf/ec2/messages"
          },
          {
            "file_path": "/opt/app/logs/*.log",
            "log_group_name": "/greenleaf/app/medusa"
          }
        ]
      }
    }
  }
}
EOF
amazon-cloudwatch-agent-ctl -a start -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json
```

---

### 5.3. Pas de plan de Disaster Recovery (DR)

**üü† Le Probl√®me :**
En cas de panne totale de la r√©gion Paris (rare mais possible), comment reprendre l'activit√© ?

**‚úÖ La Solution (√† moyen terme) :**

| Composant | Strat√©gie DR |
|:----------|:-------------|
| **Code** | Git (GitHub) - Disponible partout |
| **Infra** | Terraform - Reproductible en 30 min |
| **DB** | Cross-Region Snapshot Copy vers eu-west-1 |
| **M√©dias** | Cloudflare R2 (multi-r√©gion natif) |

```hcl
# Copie automatique des snapshots vers une autre r√©gion
resource "aws_db_instance_automated_backups_replication" "dr" {
  source_db_instance_arn = aws_db_instance.prod.arn
  kms_key_id             = aws_kms_key.dr.arn
  # R√©gion de destination configur√©e via provider
}
```

**RTO/RPO :**
- **RPO (Recovery Point Objective)** : 1 heure (fr√©quence des snapshots)
- **RTO (Recovery Time Objective)** : 2 heures (temps de reconstruction)

---

## 6. Failles Performance / Gestion de la Charge

### 6.1. Scaling Trop Lent (Cold Start)

**üî¥ Le Probl√®me :**
L'Auto Scaling Group met **3-5 minutes** √† lancer une nouvelle instance (d√©marrage EC2 + Docker pull + d√©marrage app). Pendant un pic soudain (Flash Sale, pub TV), le site peut √™tre satur√© avant que les nouvelles instances n'arrivent.

**‚úÖ La Solution :**

1. **Warm Pool (Instances pr√©-chauff√©es)** :
```hcl
resource "aws_autoscaling_group" "app" {
  # ...
  
  warm_pool {
    pool_state                  = "Stopped"
    min_size                    = 1
    max_group_prepared_capacity = 2
  }
}
```
Les instances sont cr√©√©es √† l'avance mais arr√™t√©es. Au moment du scaling, elles d√©marrent en **30 secondes** au lieu de 5 minutes.

2. **Scaling Pr√©dictif** :
```hcl
resource "aws_autoscaling_policy" "predictive" {
  name                   = "predictive-scaling"
  autoscaling_group_name = aws_autoscaling_group.app.name
  policy_type            = "PredictiveScaling"

  predictive_scaling_configuration {
    metric_specification {
      target_value = 60
      predefined_scaling_metric_specification {
        predefined_metric_type = "ASGAverageCPUUtilization"
      }
    }
    mode                         = "ForecastAndScale"
    scheduling_buffer_time       = 300  # 5 min avant
  }
}
```

**D√©fense orale :**
> "Nous utilisons un Warm Pool pour r√©duire le temps de scaling de 5 minutes √† 30 secondes. Pour les pics pr√©visibles (soldes), nous augmentons manuellement la capacit√© la veille."

---

### 6.2. Base de Donn√©es = Goulot d'√âtranglement

**üî¥ Le Probl√®me :**
Le RDS `db.t3.medium` a des limites :
- **Max connections** : ~100 connexions simultan√©es
- **IOPS** : 3000 (burst) puis throttling
- Si toutes les instances EC2 ouvrent des connexions, la DB sature.

**‚úÖ La Solution :**

1. **Connection Pooling avec PgBouncer** :
```yaml
# docker-compose.yml
services:
  pgbouncer:
    image: edoburu/pgbouncer:latest
    environment:
      DATABASE_URL: postgres://user:pass@rds-endpoint:5432/medusa
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
    ports:
      - "6432:6432"
```
Medusa se connecte √† PgBouncer (1000 connexions) qui maintient 20 connexions vers RDS.

2. **Read Replicas** (si budget permet) :
```hcl
resource "aws_db_instance" "replica" {
  replicate_source_db = aws_db_instance.main.identifier
  instance_class      = "db.t3.small"
  # Lecture seule pour les requ√™tes catalogue
}
```

3. **Monitoring des connexions** :
```hcl
resource "aws_cloudwatch_metric_alarm" "db_connections" {
  alarm_name          = "rds-connections-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "DatabaseConnections"
  namespace           = "AWS/RDS"
  period              = 300
  statistic           = "Average"
  threshold           = 80  # Alerte √† 80% de la limite
  alarm_actions       = [aws_sns_topic.alerts.arn]
}
```

---

### 6.3. Redis Satur√© (M√©moire / Connexions)

**üü† Le Probl√®me :**
Le `cache.t3.micro` a seulement **0.5 GB de RAM**. Si trop de sessions sont stock√©es ou si les √©v√©nements Medusa s'accumulent, Redis peut :
- Rejeter les nouvelles √©critures
- √âvincer des donn√©es importantes (sessions = d√©connexion utilisateurs)

**‚úÖ La Solution :**

1. **√âviction Policy appropri√©e** :
```hcl
resource "aws_elasticache_parameter_group" "redis" {
  family = "redis7"
  name   = "greenleaf-redis"

  parameter {
    name  = "maxmemory-policy"
    value = "volatile-lru"  # √âvince les cl√©s avec TTL en premier
  }
}
```

2. **TTL obligatoire sur toutes les cl√©s** :
```javascript
// Dans Medusa/Node.js
await redis.set("session:xyz", data, "EX", 3600);  // Expire en 1h
```

3. **Alarme m√©moire** :
```hcl
resource "aws_cloudwatch_metric_alarm" "redis_memory" {
  alarm_name          = "redis-memory-high"
  comparison_operator = "GreaterThanThreshold"
  metric_name         = "DatabaseMemoryUsagePercentage"
  namespace           = "AWS/ElastiCache"
  threshold           = 80
  # ...
}
```

4. **Upgrade si n√©cessaire** : Passer √† `cache.t3.small` (1.5 GB) = +8$/mois.

---

### 6.4. Thundering Herd (Effet de Troupeau)

**üî¥ Le Probl√®me :**
Sc√©nario catastrophe :
1. Le cache Cloudflare expire
2. 10,000 utilisateurs demandent la m√™me page
3. 10,000 requ√™tes arrivent simultan√©ment sur l'ALB
4. Les EC2 et RDS sont submerg√©s

**‚úÖ La Solution :**

1. **Stale-While-Revalidate sur Cloudflare** :
```
Cache-Control: public, max-age=60, stale-while-revalidate=3600
```
Cloudflare sert le cache p√©rim√© pendant qu'il rafra√Æchit en arri√®re-plan.

2. **Request Coalescing (Cache-c√¥t√© app)** :
```javascript
// Avec une lib comme "swr" ou "dataloader"
const productLoader = new DataLoader(async (ids) => {
  // Une seule requ√™te DB pour N demandes identiques
  return await db.products.findMany({ where: { id: { in: ids } } });
});
```

3. **Rate Limiting Cloudflare** :
```
R√®gle WAF : Si > 100 req/sec de la m√™me IP ‚Üí Challenge CAPTCHA
```

---

### 6.5. Pas de Load Testing Avant Production

**üî¥ Le Probl√®me :**
On ne conna√Æt pas la capacit√© r√©elle du syst√®me. Combien d'utilisateurs simultan√©s avant que √ßa plante ?

**‚úÖ La Solution :**

1. **Test de charge avec k6** :
```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // Mont√©e √† 100 users
    { duration: '5m', target: 100 },   // Maintien
    { duration: '2m', target: 200 },   // Mont√©e √† 200 users
    { duration: '5m', target: 200 },   // Maintien
    { duration: '2m', target: 0 },     // Descente
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],  // 95% des requ√™tes < 500ms
    http_req_failed: ['rate<0.01'],    // < 1% d'erreurs
  },
};

export default function () {
  const res = http.get('https://greenleaf.com/');
  check(res, { 'status is 200': (r) => r.status === 200 });
  sleep(1);
}
```

2. **Ex√©cution** :
```bash
k6 run load-test.js
```

3. **Documenter les r√©sultats** :
| M√©trique | R√©sultat |
|:---------|:---------|
| Max Users Simultan√©s | 200 |
| P95 Latency | 450ms |
| Erreurs | 0.5% |
| Bottleneck | RDS (CPU 95%) |

**D√©fense orale :**
> "Nous avons effectu√© un test de charge avec k6. Le syst√®me supporte 200 utilisateurs simultan√©s avec un P95 < 500ms. Au-del√†, le goulot d'√©tranglement est la base de donn√©es. La solution est d'ajouter un Read Replica."

---

## 7. Matrice de Risques - R√©capitulatif

| Faille | Probabilit√© | Impact | Risque | Contre-mesure | Priorit√© |
|:-------|:------------|:-------|:-------|:--------------|:---------|
| NAT Instance SPOF | Moyenne | Moyen | üü° | ASG auto-healing | P2 |
| ALB expos√© | Faible | √âlev√© | üü° | SG Cloudflare-only | P1 |
| Secrets en clair | Moyenne | Critique | üî¥ | SSM Parameter Store | **P0** |
| RDS sans least privilege | Moyenne | Critique | üî¥ | User DB limit√© | **P0** |
| Redis sans auth | Faible | √âlev√© | üü† | AUTH token | P1 |
| Backups non test√©s | √âlev√©e | Critique | üî¥ | Test mensuel | **P0** |
| Pas de DR | Tr√®s faible | Critique | üü° | Cross-region backup | P3 |
| **Scaling lent** | **Moyenne** | **√âlev√©** | **üî¥** | **Warm Pool** | **P1** |
| **DB satur√©e** | **Moyenne** | **Critique** | **üî¥** | **PgBouncer** | **P0** |
| **Redis satur√©** | **Faible** | **Moyen** | **üü°** | **√âviction + TTL** | **P2** |
| **Thundering Herd** | **Faible** | **√âlev√©** | **üü†** | **Stale-while-revalidate** | **P1** |
| **Pas de load test** | **√âlev√©e** | **√âlev√©** | **üî¥** | **k6 avant prod** | **P0** |

---

## 8. R√©ponses aux Questions du Jury

**Q: "Votre NAT Instance, c'est un SPOF ?"**
> "Non, elle est dans un ASG de taille 1 avec auto-healing. En cas de panne, AWS la remplace en 2 minutes. Et le trafic client ne passe pas par la NAT, seulement le trafic sortant des serveurs."

**Q: "Et si Cloudflare tombe ?"**
> "C'est un risque accept√©. Cloudflare a un SLA de 100% sur son plan Pro. En backup, on peut basculer le DNS directement vers l'ALB en 5 minutes, mais on perd le WAF."

**Q: "Comment vous g√©rez les secrets ?"**
> "Les credentials sont stock√©s dans AWS SSM Parameter Store avec chiffrement KMS. Les EC2 les r√©cup√®rent au d√©marrage via leur IAM Role. Rien n'est en dur dans le code."

**Q: "Vous avez test√© la restauration de backup ?"**
> "Nous avons une proc√©dure document√©e de test mensuel. Le dernier test a restaur√© la base en PREPROD en 15 minutes."

**Q: "Que se passe-t-il en cas de pic de trafic soudain ?"**
> "L'Auto Scaling r√©agit en 30 secondes gr√¢ce au Warm Pool. Pour un pic extr√™me, Cloudflare absorbe le trafic statique et son rate limiting prot√®ge l'origine. Nous avons test√© jusqu'√† 200 users simultan√©s avec k6."

**Q: "Comment vous √©vitez que la base de donn√©es sature ?"**
> "Nous utilisons PgBouncer pour le connection pooling. Au lieu de 100 connexions directes vers RDS, nous en avons 20 qui sont partag√©es entre toutes les instances. Nous avons aussi une alarme CloudWatch √† 80% de la limite."

**Q: "Votre Redis est petit, √ßa ne pose pas probl√®me ?"**
> "Nous avons configur√© une politique d'√©viction volatile-lru et toutes nos cl√©s ont un TTL. Une alarme nous pr√©vient √† 80% de m√©moire utilis√©e. Si n√©cessaire, on peut upgrader en 5 minutes via Terraform."

---
